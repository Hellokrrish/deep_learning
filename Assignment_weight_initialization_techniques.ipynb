{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/kitLRz6FHMLGtT1rqa3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hellokrrish/deep_learning/blob/main/Assignment_weight_initialization_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VseWWLaSnkUy"
      },
      "outputs": [],
      "source": [
        "'''1.What is the vanishing gradient problem in deep neural networks? How does it affect training\n",
        "\t 2. Explain how Xavier initialization addresses the vanishing gradient problem&\n",
        "   3.What are some common activation functions that are prone to causing vanishing gradients\n",
        "   4.Define the exploding gradient problem in deep neural networks. How does it impact training\n",
        "   5.What is the role of proper weight initialization in training deep neural networks\n",
        "   6. Explain the concept of batch normalization and its impact on weight initialization techniques&\n",
        "   7.Implement He initialization in Python using TensorFlow or PyTorch.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''1. Vanishing Gradient Problem\n",
        "\n",
        "Definition: In deep neural networks, the gradients of the loss function with respect to the weights of the earlier layers can become extremely small during backpropagation. This phenomenon is known as the vanishing gradient problem.\n",
        "Impact on Training:\n",
        "Slow Training: As gradients become very small, the weights of the earlier layers update very slowly. This significantly slows down the training process.\n",
        "Difficulty in Learning: In extreme cases, the weights of the earlier layers may essentially stop updating, preventing the network from learning effective representations.\n",
        "Convergence Issues: The training process may get stuck in local minima or fail to converge altogether.\n",
        "2. Xavier Initialization\n",
        "\n",
        "Addressing Vanishing Gradients: Xavier initialization aims to distribute the initial weights of the network in a way that ensures that the variance of the activations remains constant across layers. This helps to prevent the gradients from becoming too small or too large.\n",
        "How it Works:\n",
        "For a layer with input units and output units, Xavier initialization suggests initializing the weights with values drawn from a uniform distribution:\n",
        "[-sqrt(6 / (input_units + output_units)), sqrt(6 / (input_units + output_units))]\n",
        "Or from a normal distribution with:\n",
        "mean = 0\n",
        "standard deviation = sqrt(2 / (input_units + output_units))\n",
        "3. Activation Functions Prone to Vanishing Gradients\n",
        "\n",
        "Sigmoid: The sigmoid function has a limited output range (0 to 1) and saturates easily. When neurons saturate, their gradients become very small, contributing to the vanishing gradient problem.\n",
        "Tanh: Similar to sigmoid, the tanh function also saturates, leading to vanishing gradients.\n",
        "4. Exploding Gradient Problem\n",
        "\n",
        "Definition: The opposite of the vanishing gradient problem. In some cases, the gradients can become extremely large during backpropagation, leading to instability in the training process.\n",
        "Impact on Training:\n",
        "Instability: Large gradients can cause the weights to update drastically, leading to instability and divergence of the training process.\n",
        "Oscillations: The weights may oscillate wildly, making it difficult for the network to converge to a good solution.\n",
        "5. Role of Proper Weight Initialization\n",
        "\n",
        "Crucial for Training: Proper weight initialization plays a critical role in the successful training of deep neural networks.\n",
        "Stability: Good initialization helps to stabilize the training process by ensuring that the gradients remain within a reasonable range.\n",
        "Convergence: Appropriate initialization can help the network converge faster and achieve better performance.\n",
        "6. Batch Normalization and Weight Initialization\n",
        "\n",
        "Batch Normalization: This technique normalizes the activations of a layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
        "Impact on Weight Initialization: Batch normalization can reduce the sensitivity of the network to the initial weights. This makes the training process more robust to different initialization schemes.\n",
        "Interaction: Batch normalization can sometimes make the network less sensitive to the specific choice of weight initialization.'''"
      ],
      "metadata": {
        "id": "V8vm71eUoBYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Apply He initialization to the first layer\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=math.sqrt(5))\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "C7tZoFkyoOzF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUBYAWPuoSOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}