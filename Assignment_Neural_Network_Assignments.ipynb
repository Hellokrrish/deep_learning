{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN97eAX1pfIRUDii6SH827h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hellokrrish/deep_learning/blob/main/Assignment_Neural_Network_Assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVbj_gw4jIPt"
      },
      "outputs": [],
      "source": [
        "'''Introduction to Deep Learning Assignment questions.\n",
        "\n",
        "\n",
        "\n",
        "1.Explain what deep learning is and discuss its significance in the broader field of artificial intelligence.\n",
        "\n",
        "\n",
        "2. List and explain the fundamental components of artificial neural networks. 3.Discuss the roles of\n",
        "neurons, connections, weights, and biases.\n",
        "\n",
        "\n",
        "4.Illustrate the architecture of an artificial neural network. Provide an example to explain the flow of\n",
        "information through the network.\n",
        "\n",
        "\n",
        "5.Outline the perceptron learning algorithm. Describe how weights are adjusted during the learning\n",
        "process.\n",
        "\n",
        "\n",
        "6.Discuss the importance of activation functions in the hidden layers of a multi-layer perceptron. Provide\n",
        "examples of commonly used activation functions\n",
        "\n",
        "\n",
        "Various Neural Network Architect Overview Assignments\n",
        "\n",
        "\n",
        "1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the\n",
        "activation function?\n",
        "\n",
        "\n",
        "2 Explain the role of convolutional layers in CNN. Why are pooling layers commonly used, and what do they\n",
        "achieve?\n",
        "\n",
        "\n",
        "3 What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural\n",
        "networks? How does an RNN handle sequential data?\n",
        "\n",
        "\n",
        "4 . Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the\n",
        "vanishing gradient problem?\n",
        "\n",
        "\n",
        "5 Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is\n",
        "the training objective for each?\n",
        "\n",
        "\n",
        "Activation functions assignment questions\n",
        "\n",
        "\n",
        "1. Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear\n",
        "activation functions. Why are nonlinear activation functions preferred in hidden layers?\n",
        "\n",
        "\n",
        "2. Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it\n",
        "commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages and\n",
        "potential challenges.What is the purpose of the Tanh activation function? How does it differ from the\n",
        "Sigmoid activation function?\n",
        "\n",
        "\n",
        "3.Discuss the significance of activation functions in the hidden layers of a neural network.\n",
        "\n",
        "\n",
        "4.Explain the choice of activation functions for different types of problems (e.g., classification, regression)\n",
        "in the output layer.\n",
        "\n",
        "\n",
        "5. Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network\n",
        "architecture. Compare their effects on convergence and performance.\n",
        "\n",
        "Loss Functions assignment questions\n",
        "\n",
        "\n",
        "1.Explain the concept of a loss function in the context of deep learning. Why are loss functions important in\n",
        "training neural networks?\n",
        "\n",
        "\n",
        "2.Compare and contrast commonly used loss functions in deep learning, such as Mean Squared Error (MSE),\n",
        "Binary Cross-Entropy, and Categorical Cross-Entropy. When would you choose one over the other?\n",
        "\n",
        "\n",
        "3.Discuss the challenges associated with selecting an appropriate loss function for a given deep learning\n",
        "task. How might the choice of loss function affect the training process and model performance?\n",
        "\n",
        "\n",
        "4.Implement a neural network for binary classification using TensorFlow or PyTorch. Choose an appropriate\n",
        "loss function for this task and explain your reasoning. Evaluate the performance of your model on a test\n",
        "dataset.\n",
        "\n",
        "\n",
        "5.Consider a regression problem where the target variable has outliers. How might the choice of loss\n",
        "function impact the model's ability to handle outliers? Propose a strategy for dealing with outliers in the\n",
        "context of deep learning.\n",
        "\n",
        "\n",
        "6.Explore the concept of weighted loss functions in deep learning. When and why might you use weighted\n",
        "loss functions? Provide examples of scenarios where weighted loss functions could be beneficial.\n",
        "\n",
        "\n",
        "7.Investigate how the choice of activation function interacts with the choice of loss function in deep learning\n",
        "models. Are there any combinations of activation functions and loss functions that are particularly effective\n",
        "or problematic?\n",
        "\n",
        "\n",
        "Optimizers\n",
        "\n",
        "\n",
        "1.Define the concept of optimization in the context of training neural networks. Why are optimizers important\n",
        "for the training process?\n",
        "\n",
        "\n",
        "2.Compare and contrast commonly used optimizers in deep learning, such as Stochastic Gradient Descent\n",
        "(SGD), Adam, RMSprop, and AdaGrad. What are the key differences between these optimizers, and when\n",
        "might you choose one over the others?\n",
        "\n",
        "\n",
        "3.Discuss the challenges associated with selecting an appropriate optimizer for a given deep learning task.\n",
        "How might the choice of optimizer affect the training dynamics and convergence of the neural network?\n",
        "\n",
        "\n",
        "4. Implement a neural network for image classification using TensorFlow or PyTorch. Experiment with\n",
        "different optimizers and evaluate their impact on the training process and model performance. Provide\n",
        "insights into the advantages and disadvantages of each optimizer.\n",
        "\n",
        "\n",
        "5. Investigate the concept of learning rate scheduling and its relationship with optimizers in deep learning.\n",
        "How does learning rate scheduling influence the training process and model convergence? Provide\n",
        "examples of different learning rate scheduling techniques and their practical implications.\n",
        "\n",
        "\n",
        "6. Explore the role of momentum in optimization algorithms, such as SGD with momentum and Adam. How\n",
        "does momentum affect the optimization process, and under what circumstances might it be beneficial or\n",
        "detrimental?\n",
        "\n",
        "\n",
        "7. Discuss the importance of hyperparameter tuning in optimizing deep learning models. How do\n",
        "hyperparameters, such as learning rate and momentum, interact with the choice of optimizer? Propose a\n",
        "systematic approach for hyperparameter tuning in the context of deep learning optimization.\n",
        "\n",
        "Assignment Questions on Forward and Backward Propagation\n",
        "\n",
        "\n",
        "1. Explain the concept of forward propagation in a neural network.\n",
        "\n",
        "2. What is the purpose of the activation function in forward propagation?\n",
        "\n",
        "3. Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
        "\n",
        "4. What is the purpose of the chain rule in backpropagation?\n",
        "\n",
        "5 .Implement the forward propagation process for a simple neural network with one hidden layer using\n",
        "NumPy.\n",
        "\n",
        "\n",
        "Assignment on weight initialization techniques\n",
        "\n",
        "\n",
        "1 What is the vanishing gradient problem in deep neural networks? How does it affect training?\n",
        "\n",
        "2. Explain how Xavier initialization addresses the vanishing gradient problem.\n",
        "\n",
        "3. What are some common activation functions that are prone to causing vanishing gradients?\n",
        "\n",
        "4. Define the exploding gradient problem in deep neural networks. How does it impact training?\n",
        "\n",
        "5. What is the role of proper weight initialization in training deep neural networks?\n",
        "\n",
        "6. Explain the concept of batch normalization and its impact on weight initialization techniques.\n",
        "\n",
        "7. Implement He initialization in Python using TensorFlow or PyTorch.\n",
        "\n",
        "\n",
        "Assignment questions on Vanishing Gradient Problem:\n",
        "\n",
        "\n",
        "1.Define the vanishing gradient problem and the exploding gradient problem in the context of training deep\n",
        "neural networks. What are the underlying causes of each problem?\n",
        "\n",
        "\n",
        "2.Discuss the implications of the vanishing gradient problem and the exploding gradient problem on the\n",
        "training process of deep neural networks. How do these problems affect the convergence and stability of the\n",
        "optimization process?\n",
        "\n",
        "\n",
        "3.Explore the role of activation functions in mitigating the vanishing gradient problem and the exploding\n",
        "gradient problem. How do activation functions such as ReLU, sigmoid, and tanh influence gradient flow\n",
        "during backpropagation? '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 1. Deep Learning\n",
        "\n",
        "Definition: Deep learning is a subfield of machine learning that utilizes artificial neural networks with multiple layers (deep networks) to extract higher-level features from raw data. These networks can learn complex patterns and representations, enabling them to excel in tasks like image recognition, natural language processing, and speech recognition.\n",
        "Significance in AI: Deep learning has revolutionized many areas of artificial intelligence. Its ability to learn complex representations from data has led to significant breakthroughs in various fields, including computer vision, natural language processing, and robotics. Deep learning models have achieved state-of-the-art performance on a wide range of tasks, surpassing traditional machine learning methods in many cases.\n",
        "2. Fundamental Components of Artificial Neural Networks\n",
        "\n",
        "Neurons: The basic building blocks of neural networks. Each neuron receives input from other neurons, performs a weighted sum of the inputs, and then applies an activation function to produce an output.\n",
        "Connections: The links between neurons. Each connection has a weight associated with it, which determines the strength of the connection.\n",
        "Weights: The weights of the connections between neurons. They are learned during the training process and determine the strength of the connection between neurons.\n",
        "Biases: A constant value added to the weighted sum of inputs before the activation function is applied. Biases allow the neuron to shift its activation function.\n",
        "3. Architecture of an Artificial Neural Network\n",
        "\n",
        "Layers: Neural networks are typically organized into layers:\n",
        "Input layer: Receives the input data.\n",
        "Hidden layers: Perform computations on the input data and extract higher-level features.\n",
        "Output layer: Produces the final output of the network.\n",
        "Information Flow:\n",
        "Input data is fed into the input layer.\n",
        "The input data is passed through the hidden layers, where each neuron performs a weighted sum of its inputs and applies an activation function.\n",
        "The output of the final hidden layer is passed to the output layer, which produces the final output of the network.\n",
        "4. Perceptron Learning Algorithm\n",
        "\n",
        "Goal: To adjust the weights of the connections between neurons to minimize the error between the predicted output and the actual output.1\n",
        "1.\n",
        "saturncloud.io\n",
        "saturncloud.io\n",
        "Process:\n",
        "Initialize the weights of the connections to random values.\n",
        "For each training example:\n",
        "Calculate the predicted output of the network.\n",
        "Calculate the error between the predicted output and the actual output.\n",
        "Adjust the weights of the connections based on the error. The weight adjustment is typically done using gradient descent.\n",
        "5. Activation Functions in Hidden Layers\n",
        "\n",
        "Role: Introduce non-linearity into the network. Without non-linearity, the network would only be able to learn linear relationships between the input and output. Activation functions allow the network to learn more complex patterns in the data.\n",
        "Examples:\n",
        "Sigmoid: Outputs a value between 0 and 1.\n",
        "ReLU (Rectified Linear Unit): Outputs the input directly if it's positive, and 0 otherwise.\n",
        "Tanh: Outputs a value between -1 and 1.\n",
        "6. Forward and Backward Propagation\n",
        "\n",
        "Forward Propagation: The process of feeding input data through the network to generate an output.\n",
        "Backward Propagation: The process of calculating the gradients of the loss function with respect to the weights of the network. These gradients are then used to update the weights using an optimization algorithm like gradient descent.\n",
        "7. Vanishing Gradient Problem\n",
        "\n",
        "Definition: As the number of layers in a deep neural network increases, the gradients of the loss function with respect to the weights of the earlier layers can become very small. This can make it difficult for the network to learn.\n",
        "Impact: The vanishing gradient problem can slow down the training process and prevent the network from learning effectively.\n",
        "Mitigation:\n",
        "Weight Initialization Techniques: Xavier and He initialization can help to mitigate the vanishing gradient problem.\n",
        "Activation Functions: ReLU can help to mitigate the vanishing gradient problem compared to sigmoid and tanh.\n",
        "Batch Normalization: Can help to stabilize the training process and improve the performance of deep neural networks.'''"
      ],
      "metadata": {
        "id": "XZ1atQDljb7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}